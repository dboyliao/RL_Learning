{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandits = [0.2, 0, -0.5, -1]\n",
    "num_bandits = len(bandits)\n",
    "\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "# the 4th bandit is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 bandits: [-1.  0.  0.  0.]\n",
      "loss: 0.0\n",
      "policy: [ 0.99900001  1.          1.          1.        ]\n",
      "Running reward for the 4 bandits: [-2.  9.  0.  2.]\n",
      "loss: 0.009923764504492283\n",
      "policy: [ 0.99799901  1.00898302  1.          1.00199902]\n",
      "Running reward for the 4 bandits: [-2.  5.  1.  3.]\n",
      "loss: -0.004029489122331142\n",
      "policy: [ 0.99799901  1.00503361  1.001001    1.00299704]\n",
      "Running reward for the 4 bandits: [ -3.  13.   0.   3.]\n",
      "loss: -0.01192594412714243\n",
      "policy: [ 0.996997    1.01298547  1.00000203  1.00299704]\n",
      "Running reward for the 4 bandits: [ -3.  15.   2.   3.]\n",
      "loss: -0.013897789642214775\n",
      "policy: [ 0.996997    1.01498103  1.00200105  1.00299704]\n",
      "Running reward for the 4 bandits: [ -3.  13.   2.   3.]\n",
      "loss: -0.011971530504524708\n",
      "policy: [ 0.996997    1.0130316   1.002002    1.00299704]\n",
      "Running reward for the 4 bandits: [ -3.  23.   2.   5.]\n",
      "loss: -0.02166229672729969\n",
      "policy: [ 0.996997    1.02287722  1.00200295  1.0049901 ]\n",
      "Running reward for the 4 bandits: [ -6.  17.   2.   6.]\n",
      "loss: 0.01783793419599533\n",
      "policy: [ 0.99398488  1.0170157   1.00200295  1.00598514]\n",
      "Running reward for the 4 bandits: [ -6.  16.   3.   6.]\n",
      "loss: -0.014955893158912659\n",
      "policy: [ 0.99398589  1.01605344  1.00300097  1.00598621]\n",
      "Running reward for the 4 bandits: [ -8.   4.   5.  18.]\n",
      "loss: -0.016728365793824196\n",
      "policy: [ 0.99197274  1.0041883   1.00499403  1.01785243]\n",
      "Running reward for the 4 bandits: [ -7.   4.   6.  54.]\n",
      "loss: -0.05040370300412178\n",
      "policy: [ 0.99298185  1.00418937  1.00598907  1.0526464 ]\n",
      "Running reward for the 4 bandits: [ -7.   3.   6.  93.]\n",
      "loss: 0.08618013560771942\n",
      "policy: [ 0.99298185  1.0031935   1.00598907  1.08908522]\n",
      "Running reward for the 4 bandits: [  -7.    3.    6.  127.]\n",
      "loss: 0.1140235885977745\n",
      "policy: [ 0.99298286  1.00319445  1.00598907  1.11988628]\n",
      "Running reward for the 4 bandits: [  -9.    3.    7.  150.]\n",
      "loss: -0.1304820328950882\n",
      "policy: [ 0.99096769  1.00319445  1.00698411  1.14025509]\n",
      "Running reward for the 4 bandits: [  -6.    2.    7.  186.]\n",
      "loss: 0.1589415818452835\n",
      "policy: [ 0.99399197  1.00219762  1.00698411  1.1714164 ]\n",
      "Running reward for the 4 bandits: [  -8.    2.    6.  215.]\n",
      "loss: -0.17822463810443878\n",
      "policy: [ 0.99197882  1.00219762  1.0059911   1.19593048]\n",
      "Running reward for the 4 bandits: [  -9.    2.    6.  246.]\n",
      "loss: -0.1994822919368744\n",
      "policy: [ 0.99097073  1.00219762  1.00599205  1.2215898 ]\n",
      "Running reward for the 4 bandits: [  -8.    1.    6.  278.]\n",
      "loss: -0.220516636967659\n",
      "policy: [ 0.99197984  1.00119984  1.00599205  1.24752283]\n",
      "Running reward for the 4 bandits: [  -8.    1.    7.  303.]\n",
      "loss: -0.23635685443878174\n",
      "policy: [ 0.99198085  1.00119984  1.00698614  1.26741576]\n",
      "Running reward for the 4 bandits: [  -7.    0.   10.  334.]\n",
      "loss: -0.25532394647598267\n",
      "policy: [ 0.99298894  1.00020099  1.00996232  1.29165435]\n",
      "\n",
      "The agent think bandit 4 is the best\n",
      "The agent is right!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "policy = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(policy, 0)\n",
    "\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "# it looks like tf.slice is equivalent to [] slice operator\n",
    "# responsible_policy = tf.slice(policy, action_holder, [1])\n",
    "responsible_policy = policy[action_holder[0]]\n",
    "loss = tf.reduce_sum(-(tf.log(responsible_policy)*reward_holder))\n",
    "train = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# Training\n",
    "num_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "e = 0.1\n",
    "np.random.seed(1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        \n",
    "        reward = pullBandit(bandits[action])\n",
    "        _, ww, l = sess.run([train, policy, loss], \n",
    "                            feed_dict={reward_holder:[reward], action_holder:[action]})\n",
    "        \n",
    "        total_reward[action] += reward\n",
    "        if episode % 50 == 0:\n",
    "            print(\"Running reward for the {} bandits: {}\".format(num_bandits, total_reward))\n",
    "            print(\"loss: {}\".format(l))\n",
    "            print(\"policy: {}\".format(ww))\n",
    "print()\n",
    "print(\"The agent think bandit {} is the best\".format(np.argmax(ww)+1))\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print(\"The agent is right!\")\n",
    "else:\n",
    "    print(\"The agent is wrong.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repeat above chunk for multiple time, you will find out that sometime the agent will stuck in bad policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
