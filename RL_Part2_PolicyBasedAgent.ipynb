{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- [Blog](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724)\n",
    "  - **key idea**: Collect rewards and use these experiences to update the NN all at once.\n",
    "  - It will force the NN to take account for the entire trace of actions (rollouts, experience traces). Note that these rewards for all the actions need to be discounted before applying it to updating NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adds a fully connected layer.\n",
      "\n",
      "  `fully_connected` creates a variable called `weights`, representing a fully\n",
      "  connected weight matrix, which is multiplied by the `inputs` to produce a\n",
      "  `Tensor` of hidden units. If a `normalizer_fn` is provided (such as\n",
      "  `batch_norm`), it is then applied. Otherwise, if `normalizer_fn` is\n",
      "  None and a `biases_initializer` is provided then a `biases` variable would be\n",
      "  created and added the hidden units. Finally, if `activation_fn` is not `None`,\n",
      "  it is applied to the hidden units as well.\n",
      "\n",
      "  Note: that if `inputs` have a rank greater than 2, then `inputs` is flattened\n",
      "  prior to the initial matrix multiply by `weights`.\n",
      "\n",
      "  Args:\n",
      "    inputs: A tensor of at least rank 2 and static value for the last dimension;\n",
      "      i.e. `[batch_size, depth]`, `[None, None, None, channels]`.\n",
      "    num_outputs: Integer or long, the number of output units in the layer.\n",
      "    activation_fn: Activation function. The default value is a ReLU function.\n",
      "      Explicitly set it to None to skip it and maintain a linear activation.\n",
      "    normalizer_fn: Normalization function to use instead of `biases`. If\n",
      "      `normalizer_fn` is provided then `biases_initializer` and\n",
      "      `biases_regularizer` are ignored and `biases` are not created nor added.\n",
      "      default set to None for no normalizer function\n",
      "    normalizer_params: Normalization function parameters.\n",
      "    weights_initializer: An initializer for the weights.\n",
      "    weights_regularizer: Optional regularizer for the weights.\n",
      "    biases_initializer: An initializer for the biases. If None skip biases.\n",
      "    biases_regularizer: Optional regularizer for the biases.\n",
      "    reuse: Whether or not the layer and its variables should be reused. To be\n",
      "      able to reuse the layer scope must be given.\n",
      "    variables_collections: Optional list of collections for all the variables or\n",
      "      a dictionary containing a different list of collections per variable.\n",
      "    outputs_collections: Collection to add the outputs.\n",
      "    trainable: If `True` also add variables to the graph collection\n",
      "      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
      "    scope: Optional scope for variable_scope.\n",
      "\n",
      "  Returns:\n",
      "     The tensor variable representing the result of the series of operations.\n",
      "\n",
      "  Raises:\n",
      "    ValueError: If x has rank less than 2 or if its last dimension is not set.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(fully_connected.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gather slices from `params` according to `indices`.\n",
       "\n",
       "  `indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\n",
       "  Produces an output tensor with shape `indices.shape + params.shape[1:]` where:\n",
       "\n",
       "  ```python\n",
       "      # Scalar indices\n",
       "      output[:, ..., :] = params[indices, :, ... :]\n",
       "\n",
       "      # Vector indices\n",
       "      output[i, :, ..., :] = params[indices[i], :, ... :]\n",
       "\n",
       "      # Higher rank indices\n",
       "      output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n",
       "  ```\n",
       "\n",
       "  If `indices` is a permutation and `len(indices) == params.shape[0]` then\n",
       "  this operation will permute `params` accordingly.\n",
       "\n",
       "  <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
       "  <img style=\"width:100%\" src=\"../../images/Gather.png\" alt>\n",
       "  </div>\n",
       "\n",
       "  Args:\n",
       "    params: A `Tensor`.\n",
       "    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
       "    validate_indices: An optional `bool`. Defaults to `True`.\n",
       "    name: A name for the operation (optional).\n",
       "\n",
       "  Returns:\n",
       "    A `Tensor`. Has the same type as `params`.\n",
       "  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_markdown\n",
    "display_markdown(tf.gather.__doc__, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Constructs symbolic partial derivatives of sum of `ys` w.r.t. x in `xs`.\n",
       "\n",
       "  `ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`\n",
       "  is a list of `Tensor`, holding the gradients received by the\n",
       "  `ys`. The list must be the same length as `ys`.\n",
       "\n",
       "  `gradients()` adds ops to the graph to output the partial\n",
       "  derivatives of `ys` with respect to `xs`.  It returns a list of\n",
       "  `Tensor` of length `len(xs)` where each tensor is the `sum(dy/dx)`\n",
       "  for y in `ys`.\n",
       "\n",
       "  `grad_ys` is a list of tensors of the same length as `ys` that holds\n",
       "  the initial gradients for each y in `ys`.  When `grad_ys` is None,\n",
       "  we fill in a tensor of '1's of the shape of y for each y in `ys`.  A\n",
       "  user can provide their own initial `grad_ys` to compute the\n",
       "  derivatives using a different initial gradient for each y (e.g., if\n",
       "  one wanted to weight the gradient differently for each value in\n",
       "  each y).\n",
       "\n",
       "  Args:\n",
       "    ys: A `Tensor` or list of tensors to be differentiated.\n",
       "    xs: A `Tensor` or list of tensors to be used for differentiation.\n",
       "    grad_ys: Optional. A `Tensor` or list of tensors the same size as\n",
       "      `ys` and holding the gradients computed for each y in `ys`.\n",
       "    name: Optional name to use for grouping all the gradient ops together.\n",
       "      defaults to 'gradients'.\n",
       "    colocate_gradients_with_ops: If True, try colocating gradients with\n",
       "      the corresponding op.\n",
       "    gate_gradients: If True, add a tuple around the gradients returned\n",
       "      for an operations.  This avoids some race conditions.\n",
       "    aggregation_method: Specifies the method used to combine gradient terms.\n",
       "      Accepted values are constants defined in the class `AggregationMethod`.\n",
       "\n",
       "  Returns:\n",
       "    A list of `sum(dy/dx)` for each x in `xs`.\n",
       "\n",
       "  Raises:\n",
       "    LookupError: if one of the operations between `x` and `y` does not\n",
       "      have a registered gradient function.\n",
       "    ValueError: if the arguments are invalid.\n",
       "\n",
       "  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(tf.gradients.__doc__, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate=0.99):\n",
    "    acc_rewards_discounted = np.zeros_like(rewards)\n",
    "    acc_rewards = 0\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        acc_rewards = acc_rewards*discount_rate + rewards[t]\n",
    "        acc_rewards_discounted[t] = acc_rewards\n",
    "    return acc_rewards_discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, learn_rate, num_states, num_actions, num_hidden_neurons):\n",
    "        self.state_in = tf.placeholder(shape=[None, num_states], dtype=tf.float32)\n",
    "        hidden_layer = fully_connected(self.state_in,\n",
    "                                       num_hidden_neurons,\n",
    "                                       biases_initializer=None,\n",
    "                                       activation_fn=tf.nn.relu)\n",
    "        self.output = fully_connected(hidden_layer, \n",
    "                                      num_actions,\n",
    "                                      biases_initializer=None,\n",
    "                                      activation_fn=tf.nn.softmax)\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        self.reward_holder = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        indexes = tf.range(0, tf.shape(self.output)[0])*tf.shape(self.output)[1]+self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), indexes)\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        weights = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for i, _ in enumerate(weights):\n",
    "            placeholder = tf.placeholder(tf.float32, name=\"{}_holder\".format(i))\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        self.gradients = tf.gradients(self.loss, weights)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(self.gradient_holders, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-13 16:12:27,725] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.02738802, -0.0328304 ,  0.00793568,  0.01976243])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset() # position, cart-velocity, pole-angle, pole-vel-at-tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.0\n",
      "26.17\n",
      "34.41\n",
      "47.28\n",
      "57.55\n",
      "84.1\n",
      "108.24\n",
      "146.65\n",
      "176.95\n",
      "180.59\n",
      "192.8\n",
      "188.18\n",
      "175.14\n",
      "160.03\n",
      "111.52\n",
      "118.12\n",
      "156.36\n",
      "183.76\n",
      "186.92\n",
      "168.74\n",
      "189.97\n",
      "183.44\n",
      "195.09\n",
      "181.98\n",
      "168.06\n",
      "175.55\n",
      "176.43\n",
      "146.55\n",
      "160.27\n",
      "185.0\n",
      "187.76\n",
      "196.57\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "196.74\n",
      "193.14\n",
      "198.49\n",
      "195.3\n",
      "190.96\n",
      "179.18\n",
      "186.45\n",
      "184.56\n",
      "189.01\n",
      "196.47\n",
      "198.24\n",
      "198.75\n"
     ]
    }
   ],
   "source": [
    "# Trainging agent\n",
    "tf.reset_default_graph()\n",
    "\n",
    "agent = Agent(learn_rate=1e-2, num_states=4, num_actions=2, num_hidden_neurons=8)\n",
    "\n",
    "num_episodes = 5000\n",
    "max_iters_per_episodes = 999\n",
    "update_frequency = 5 # how many episodes for an update\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "total_reward = []\n",
    "total_length = []\n",
    "    \n",
    "gradBuffer = sess.run(tf.trainable_variables())\n",
    "for idx, grad in enumerate(gradBuffer):\n",
    "    gradBuffer[idx] = grad * 0\n",
    "    \n",
    "for episode in range(num_episodes):\n",
    "    current_state = env.reset()\n",
    "    acc_reward = 0\n",
    "    episode_history = []\n",
    "    for i in range(max_iters_per_episodes):\n",
    "        action_dist = sess.run(agent.output, feed_dict={agent.state_in:[current_state]})[0]\n",
    "        action = np.random.choice(np.arange(action_dist.shape[0]), p=action_dist)\n",
    "        new_state, reward, is_doomed, _ = env.step(action)\n",
    "        episode_history.append((current_state, action, reward, new_state))\n",
    "        current_state = new_state\n",
    "        acc_reward += reward\n",
    "            \n",
    "        if is_doomed:\n",
    "            episode_history = np.array(episode_history)\n",
    "            episode_history[:,2] = discount_rewards(episode_history[:, 2])\n",
    "            feed_dict = {agent.reward_holder:episode_history[:,2],\n",
    "                         agent.action_holder:episode_history[:,1],\n",
    "                         agent.state_in:np.vstack(episode_history[:,0])}\n",
    "            grads = sess.run(agent.gradients, feed_dict=feed_dict)\n",
    "            for idx, grad in enumerate(grads):\n",
    "                gradBuffer[idx] += grad\n",
    "                \n",
    "            if episode % update_frequency == 0 and episode != 0:\n",
    "                feed_dict = dict(zip(agent.gradient_holders, gradBuffer))\n",
    "                _ = sess.run(agent.train_op, feed_dict=feed_dict)\n",
    "                for idx, grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[idx] = grad*0\n",
    "            total_reward.append(acc_reward)\n",
    "            total_length.append(i)\n",
    "            break\n",
    "    if episode % 100 == 0:\n",
    "        print(np.mean(total_reward[-100:]))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "env = Monitor(gym.make(\"CartPole-v0\"), \"./exp_cartpole\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "current_state = env.reset()\n",
    "for _ in range(10000):\n",
    "    action = sess.run(agent.chosen_action, feed_dict={agent.state_in:[current_state]})[0]\n",
    "    new_state, _, is_doomed, _ = env.step(action)\n",
    "    current_state = new_state\n",
    "    if is_doomed:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "- [tf.gradients](https://www.tensorflow.org/api_docs/python/tf/gradients)\n",
    "  - [StackOverflow](https://stackoverflow.com/questions/41822308/how-tf-gradients-work-in-tensorflow)\n",
    "- [CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
